{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47638c2",
   "metadata": {},
   "source": [
    "# Time Series Analysis with PySpark\n",
    "\n",
    "This notebook demonstrates how to load, explore, and analyze time series data using PySpark. It is designed to run in the local development environment using the DevContainer setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, avg, weekofyear, year, month\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578cdd9",
   "metadata": {},
   "source": [
    "## Load CSV Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update path as per your repo structure\n",
    "data_path = \"data/input/project/raw_time_series/csv/sample_timeseries.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(data_path)\n",
    "\n",
    "# Show schema and sample data\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8077bf9",
   "metadata": {},
   "source": [
    "## Clean and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f39ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string date column to proper DateType\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Drop rows with nulls in critical fields\n",
    "df = df.dropna(subset=[\"date\", \"value\"])\n",
    "\n",
    "# Show cleaned data\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8d7af",
   "metadata": {},
   "source": [
    "## Weekly and Monthly Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f364b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df.withColumn(\"week\", weekofyear(\"date\")).withColumn(\"year\", year(\"date\")) \\\n",
    "    .groupBy(\"year\", \"week\").agg(avg(\"value\").alias(\"weekly_avg\")) \\\n",
    "    .orderBy(\"year\", \"week\")\n",
    "\n",
    "df_monthly = df.withColumn(\"month\", month(\"date\")).withColumn(\"year\", year(\"date\")) \\\n",
    "    .groupBy(\"year\", \"month\").agg(avg(\"value\").alias(\"monthly_avg\")) \\\n",
    "    .orderBy(\"year\", \"month\")\n",
    "\n",
    "df_weekly.show(5)\n",
    "df_monthly.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49500d7b",
   "metadata": {},
   "source": [
    "## Visualize Time Series Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for plotting\n",
    "pdf = df.select(\"date\", \"value\").orderBy(\"date\").toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pdf[\"date\"], pdf[\"value\"], marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Daily Time Series\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341d47",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We successfully explored a time series dataset using PySpark, applied transformations, computed weekly and monthly averages, and visualized trends using Matplotlib."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
