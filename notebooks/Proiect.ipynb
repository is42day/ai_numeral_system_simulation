{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47638c2",
   "metadata": {},
   "source": [
    "# Time Series Analysis with PySpark\n",
    "\n",
    "This notebook demonstrates how to load, explore, and analyze time series data using PySpark. It is designed to run in the local development environment using the DevContainer setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/04 06:57:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, avg, weekofyear, year, month\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578cdd9",
   "metadata": {},
   "source": [
    "## Load Parquet Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contract_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- value_source: string (nullable = true)\n",
      " |-- annotations: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-------------------+------------+--------------------+\n",
      "|        contract_id|          timestamp|              value|value_source|         annotations|\n",
      "+-------------------+-------------------+-------------------+------------+--------------------+\n",
      "| 04_02_111 _ CHR12 |2023-01-01 06:00:00|0.02591860654732236| measurement|{\"region\":\"Europe...|\n",
      "| 04 _02_111 _CHR12 |2023-01-01 17:00:00|0.07385444264936832| measurement|{\"region\":\"Europe...|\n",
      "| 04_02_111 _ CHR12 |2023-01-01 17:30:22|0.08180149515221906| measurement|{\"region\":\"Europe...|\n",
      "| 04 _02_111 _CHR12 |2023-01-01 21:30:00|0.08670661371854547| measurement|{\"region\":\"Europe...|\n",
      "|04 _ 02 _111_CHR12 |2023-01-02 00:30:00|0.03597601881331959| measurement|{\"region\":\"Europe...|\n",
      "+-------------------+-------------------+-------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uptimestamp path as per your repo structure\n",
    "data_path = \"data/input/project/raw_time_series/parquet\"\n",
    "\n",
    "# Load CSV\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).parquet(data_path)\n",
    "\n",
    "# Show schema and sample data\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8077bf9",
   "metadata": {},
   "source": [
    "## Clean and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+------------+--------------------+\n",
      "|        contract_id| timestamp|              value|value_source|         annotations|\n",
      "+-------------------+----------+-------------------+------------+--------------------+\n",
      "| 04_02_111 _ CHR12 |2023-01-01|0.02591860654732236| measurement|{\"region\":\"Europe...|\n",
      "| 04 _02_111 _CHR12 |2023-01-01|0.07385444264936832| measurement|{\"region\":\"Europe...|\n",
      "| 04_02_111 _ CHR12 |2023-01-01|0.08180149515221906| measurement|{\"region\":\"Europe...|\n",
      "| 04 _02_111 _CHR12 |2023-01-01|0.08670661371854547| measurement|{\"region\":\"Europe...|\n",
      "|04 _ 02 _111_CHR12 |2023-01-02|0.03597601881331959| measurement|{\"region\":\"Europe...|\n",
      "+-------------------+----------+-------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert string timestamp column to proper timestampType\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Drop rows with nulls in critical fields\n",
    "df = df.dropna(subset=[\"timestamp\", \"value\"])\n",
    "\n",
    "# Show cleaned data\n",
    "df.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
