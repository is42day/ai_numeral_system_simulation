{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 14:15:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.io_helpers import list_files_with_aliases, load_file_with_alias\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exercise 1 - Cleaning\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "file1: ../data/input/practice/json/part-00000-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "file2: ../data/input/practice/json/part-00001-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "file3: ../data/input/practice/json/part-00002-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Select format and folder\n",
    "# Choose between \"json\" or \"parquet\"\n",
    "data_type = \"json\"\n",
    "folder = f\"../data/input/practice/{data_type}\"\n",
    "\n",
    "aliases = list_files_with_aliases(folder, ext=data_type)\n",
    "print(\"Available files:\")\n",
    "for alias, path in aliases.items():\n",
    "    print(f\"{alias}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading file: file1 â†’ ../data/input/practice/json/part-00000-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "+----------------------------+-------+-----------+--------------------+------+-------+\n",
      "|extra                       |inactiv|nume       |ocupatie            |varsta|vechime|\n",
      "+----------------------------+-------+-----------+--------------------+------+-------+\n",
      "|[PV, EV]                    |NULL   |Andrei     |Specialist marketing|38    |13     |\n",
      "|[3D Printer, WII]           |NULL   |Alexandru  |Specialist HR       |34    |8      |\n",
      "|[AC, EV, 5G Router]         |NULL   |Adrian     |Inginer civil       |45    |23     |\n",
      "|[XBOX]                      |NULL   | Alin      |VÃ¢nzÄƒtor  retail    |26    |2      |\n",
      "|[5G Router, 3D Printer]     |NULL   |Anton      |Manager proiect     |40    |15     |\n",
      "|[EV, AC]                    |NULL   |Ana        |Muncitor alimentar  |35    |9      |\n",
      "|[PC, 3D Printer, AC]        |false  |Bogdan     |Farmacist           |50    |32     |\n",
      "|NULL                        |true   |  CÄƒtÄƒlin  |Medic primar        |20    |0      |\n",
      "|[AC, XBOX, PV]              |NULL   |Cosmin     |Farmacist           |49    |31     |\n",
      "|[XBOX, 3D Printer, EV]      |false  |Cristian   | Asistent  social   |44    |21     |\n",
      "|[3D Printer, 3D Printer, EV]|NULL   |Gabriel    |Asistent social     |44    |21     |\n",
      "|[3D Printer, AC, WII]       |false  |George     |Agent imobiliar     |46    |25     |\n",
      "|[WII, 3D Printer]           |NULL   |Gheorghe   |Specialist HR       |34    |8      |\n",
      "|[WII, 5G Router, 5G Router] |NULL   |Grigore    |Asistent  social    |44    |21     |\n",
      "|[5G Router, PV]             |NULL   |Horia      |Arhitect            |42    |18     |\n",
      "|[PC]                        |NULL   | Ilie      |VÃ¢nzÄƒtor retail     |26    |2      |\n",
      "|[XBOX]                      |NULL   | Ion       |VÃ¢nzÄƒtor retail     |27    |2      |\n",
      "|[EV, XBOX, 3D Printer]      |NULL   |Ionel      |Inginer  civil      |45    |23     |\n",
      "|NULL                        |NULL   | Iosif     |VÃ¢nzÄƒtor  retail    |26    |2      |\n",
      "|[PV, XBOX, AC]              |false  |Lucian     |Designer grafic     |49    |30     |\n",
      "+----------------------------+-------+-----------+--------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load data using alias\n",
    "alias = \"file1\"  # Change to file2, file3, etc.\n",
    "df = load_file_with_alias(spark, folder, alias=alias, ext=data_type)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def remove_duplicates(val):\n",
    "    if not val:\n",
    "        return None\n",
    "    if isinstance(val, list):\n",
    "        cleaned = [x.strip() for x in val]\n",
    "        return ', '.join(sorted(set(cleaned)))\n",
    "    elif isinstance(val, str):\n",
    "        parts = [x.strip() for x in val.split(',')]\n",
    "        return ', '.join(sorted(set(parts)))\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "remove_dupes_udf = udf(remove_duplicates, StringType())\n",
    "df = df.withColumn(\"extra\", remove_dupes_udf(\"extra\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned output to: ../data/output/cleaned_file1.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Save cleaned output\n",
    "output_ext = \"parquet\" if data_type == \"parquet\" else \"json\"\n",
    "output_path = f\"../data/output/cleaned_{alias}.{output_ext}\"\n",
    "\n",
    "if output_ext == \"json\":\n",
    "    df.write.mode(\"overwrite\").json(output_path)\n",
    "else:\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"âœ… Saved cleaned output to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
