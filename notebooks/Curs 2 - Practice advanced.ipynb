{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in terminal to start JupiterLab : jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [notebook_setup] Project root already in sys.path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/29 08:44:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Available input files:\n",
      "file1: /app/data/input/practice/json/part-00000-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "file2: /app/data/input/practice/json/part-00001-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "file3: /app/data/input/practice/json/part-00002-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "📥 Loading file: file1 → /app/data/input/practice/json/part-00000-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "✅ Loaded file: file1 → /app/data/input/practice/json/part-00000-46e2d8a4-96d4-4b0e-a4a9-992dbceb1653-c000.json\n",
      "+-----------------------+-------+---------+--------------------+------+-------+\n",
      "|extra                  |inactiv|nume     |ocupatie            |varsta|vechime|\n",
      "+-----------------------+-------+---------+--------------------+------+-------+\n",
      "|[PV, EV]               |NULL   |Andrei   |Specialist marketing|38    |13     |\n",
      "|[3D Printer, WII]      |NULL   |Alexandru|Specialist HR       |34    |8      |\n",
      "|[AC, EV, 5G Router]    |NULL   |Adrian   |Inginer civil       |45    |23     |\n",
      "|[XBOX]                 |NULL   | Alin    |Vânzător  retail    |26    |2      |\n",
      "|[5G Router, 3D Printer]|NULL   |Anton    |Manager proiect     |40    |15     |\n",
      "+-----------------------+-------+---------+--------------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 1: Setup & Load Input Data (with dynamic alias) \n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/app\")  # 🔧 Mount base path directly in container\n",
    "\n",
    "from utils.notebook_setup import enable_project_imports\n",
    "enable_project_imports()\n",
    "\n",
    "from utils.io_helpers import list_files_with_aliases, load_file_with_alias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, lit, concat_ws, size, desc\n",
    "\n",
    "# 🚀 Start Spark\n",
    "spark = SparkSession.builder.appName(\"Pyspark - Clean & Transform\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 📂 Input config\n",
    "data_type = \"json\"  # or \"parquet\"\n",
    "input_folder = f\"/app/data/input/practice/{data_type}\"  # 🔧 Absolute path for container\n",
    "\n",
    "# 🧠 Discover input files\n",
    "aliases = list_files_with_aliases(input_folder, ext=data_type)\n",
    "print(\"📄 Available input files:\")\n",
    "for k, v in aliases.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# 🏷️ Choose which file to load\n",
    "alias = \"file1\"  # or file2, file3...\n",
    "input_path = aliases[alias]\n",
    "\n",
    "# 📦 Load using alias\n",
    "df = load_file_with_alias(spark, input_folder, alias, ext=data_type)\n",
    "print(f\"✅ Loaded file: {alias} → {input_path}\")\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 2: Select format and folder\n",
    "\n",
    "# Choose between \"json\" or \"parquet\"\n",
    "data_type = \"json\"\n",
    "folder = f\"/app/data/input/practice/{data_type}\"  \n",
    "\n",
    "aliases = list_files_with_aliases(folder, ext=data_type)\n",
    "print(\"📂 Available files:\")\n",
    "for alias, path in aliases.items():\n",
    "    print(f\"{alias}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load data using alias\n",
    "alias = \"file1\"  # Change to file2, file3, etc.\n",
    "df = load_file_with_alias(spark, folder, alias=alias, ext=data_type)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Cell 4: Clean the DataFrame\n",
    "\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, coalesce, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import unicodedata\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ 1. Clean 'nume': trim + remove diacritics\n",
    "# --------------------------------------------\n",
    "\n",
    "# UDF: Remove accents using unicodedata\n",
    "def remove_accents(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "remove_accents_udf = udf(remove_accents, StringType())\n",
    "\n",
    "# Trim and remove accents\n",
    "df = df.withColumn(\"nume\", trim(col(\"nume\")))\n",
    "df = df.withColumn(\"nume\", remove_accents_udf(\"nume\"))\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ 2. Clean 'ocupatie': trim + normalize whitespace\n",
    "# --------------------------------------------\n",
    "df = df.withColumn(\"ocupatie\", trim(regexp_replace(col(\"ocupatie\"), r\"\\s+\", \" \")))\n",
    "df = df.withColumn(\"ocupatie\", remove_accents_udf(\"ocupatie\"))\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ 3. Clean 'extra': deduplicate comma-separated entries\n",
    "# --------------------------------------------\n",
    "\n",
    "def remove_duplicates(val):\n",
    "    if not val:\n",
    "        return None\n",
    "\n",
    "    if isinstance(val, list):\n",
    "        cleaned = [str(x).strip() for x in val if x]\n",
    "    elif isinstance(val, str):\n",
    "        cleaned = [x.strip() for x in val.split(',') if x]\n",
    "    else:\n",
    "        print(f\"⚠️ Unexpected value in `extra`: {val} ({type(val)})\")\n",
    "        return str(val).strip()\n",
    "\n",
    "    return ', '.join(sorted(set(cleaned)))\n",
    "\n",
    "remove_dupes_udf = udf(remove_duplicates, StringType())\n",
    "df = df.withColumn(\"extra\", remove_dupes_udf(\"extra\"))\n",
    "df.select(\"extra\").show(10, truncate=False)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ 4. Set missing 'inactiv' to False\n",
    "# --------------------------------------------\n",
    "df = df.withColumn(\"inactiv\", coalesce(col(\"inactiv\"), lit(False)))\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ 5. Convert to Pandas for Validation & Display\n",
    "# --------------------------------------------\n",
    "df_with_vechime = df.cache()  # Optional: cache to reuse efficiently\n",
    "\n",
    "display_pdf = df.toPandas()\n",
    "display_pdf.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ Helper: Check for diacritics (non-ASCII)\n",
    "# --------------------------------------------\n",
    "def has_diacritics(text):\n",
    "    return any(unicodedata.category(c) == \"Mn\" for c in unicodedata.normalize(\"NFD\", text)) if isinstance(text, str) else False\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ Test 1: No extra whitespace in `nume` or `ocupatie`\n",
    "# --------------------------------------------\n",
    "def is_trimmed(s):\n",
    "    return isinstance(s, str) and s == s.strip()\n",
    "\n",
    "trimmed_nume = display_pdf['nume'].apply(is_trimmed)\n",
    "trimmed_ocupatie = display_pdf['ocupatie'].apply(is_trimmed)\n",
    "\n",
    "assert trimmed_nume.all(), \"❌ Some 'nume' values have leading/trailing spaces.\"\n",
    "assert trimmed_ocupatie.all(), \"❌ Some 'ocupatie' values have leading/trailing spaces.\"\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ Test 2: Diacritics removed from `nume`\n",
    "# --------------------------------------------\n",
    "diacritic_check = display_pdf['nume'].apply(lambda x: not has_diacritics(x))\n",
    "assert diacritic_check.all(), \"❌ Some 'nume' values still contain diacritics.\"\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ Test 3: `extra` column exists and is valid\n",
    "# --------------------------------------------\n",
    "# Ensure column exists\n",
    "assert \"extra\" in display_pdf.columns, \"❌ Column 'extra' is missing.\"\n",
    "\n",
    "# Ensure it's not completely empty\n",
    "non_null_extra = display_pdf['extra'].notna().sum()\n",
    "assert non_null_extra > 0, \"❌ Column 'extra' is entirely null.\"\n",
    "\n",
    "print(f\"ℹ️ 'extra' column has {non_null_extra} non-null values.\")\n",
    "\n",
    "# ✅ Check for duplicates in valid entries\n",
    "def has_duplicates(entry):\n",
    "    if not entry or not isinstance(entry, str): return False\n",
    "    parts = [p.strip() for p in entry.split(',')]\n",
    "    return len(parts) != len(set(parts))\n",
    "\n",
    "extra_dup_check = display_pdf['extra'].dropna().apply(lambda x: not has_duplicates(x))\n",
    "assert extra_dup_check.all(), \"❌ Some 'extra' values contain duplicates.\"\n",
    "\n",
    "print(\"✅ 'extra' column is present, non-null, and cleaned of duplicates.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# ✅ Test 4: `inactiv` should not have nulls\n",
    "# --------------------------------------------\n",
    "inactiv_check = display_pdf['inactiv'].apply(lambda x: isinstance(x, bool))\n",
    "assert inactiv_check.all(), \"❌ Some 'inactiv' values are missing or not boolean.\"\n",
    "\n",
    "print(\"✅ All data cleaning validations passed! 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Add 'varsta_la_contractare' and drop 'vechime'\n",
    "\n",
    "df = df.withColumn(\"varsta_la_contractare\", col(\"varsta\") - col(\"vechime\"))\n",
    "df = df.drop(\"vechime\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Add 'text_descriptiv' column\n",
    "\n",
    "df = df.withColumn(\"text_descriptiv\",\n",
    "    when(col(\"extra\").isNull() | (col(\"extra\") == \"\"), \n",
    "         concat_ws(\" \", col(\"nume\"), lit(\"în vârstă de\"), col(\"varsta\"), lit(\"ani\"), lit(\"este\"), col(\"ocupatie\")))\n",
    "    .otherwise(\n",
    "        concat_ws(\" \", col(\"nume\"), lit(\"în vârstă de\"), col(\"varsta\"), lit(\"ani\"), lit(\"este\"), col(\"ocupatie\"), lit(\"și deține:\"), col(\"extra\"))\n",
    "    )\n",
    ")\n",
    "df.select(\"nume\", \"text_descriptiv\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Cell 7: Clients with contract duration between 2 and 5 years (inclusive)\n",
    "\n",
    "df_with_vechime.filter((col(\"vechime\") >= 2) & (col(\"vechime\") <= 5)) \\\n",
    "  .select(\"nume\", \"ocupatie\", \"vechime\") \\\n",
    "  .orderBy(\"vechime\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Top 10 by age\n",
    "df.orderBy(col(\"varsta\").desc()).show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Top 5 oldest at contract time (active only)\n",
    "df.filter(col(\"inactiv\") == False) \\\n",
    "  .orderBy(col(\"varsta_la_contractare\").desc()) \\\n",
    "  .show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
