{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12712a6b-b406-4e18-95b0-8d66692e8943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "X2ptyZveGSnc",
    "outputId": "d4c303d6-cd2f-4e5b-e863-4e4d22e515e1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PySpark oferă:\n",
    "    Un obiectul de tip Spark Context, folosit pentru interacțiunea cu Spark,.\n",
    "    Un obiect de bază pentru codul SQL și citirea datelor, numit Spark Session.\n",
    "    Un obiect de lucru cu date, numit Data Frame, având o interfață similară cu cel de Pandas.\n",
    "    Funcții gata implementate pentru transformări și expresii.\n",
    "    Abilitatea de a construi propriile funcții de transformare în Python.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30374,
     "status": "ok",
     "timestamp": 1742584153104,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "uVuWHHmMGqCm",
    "outputId": "84de480b-a563-4093-b702-94b3bc2aab94"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pregătire mediu de lucru\n",
    "Stabilirea conexiunii dintre Google drive si Colab notebook\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 74543,
     "status": "ok",
     "timestamp": 1742584617638,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "RRptX9YIIJnG",
    "outputId": "485bf6fe-0fe6-40c6-aaf0-b49ce2983b03"
   },
   "outputs": [],
   "source": [
    "# Instalarea PySpark\n",
    "\n",
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://dlcdn.apache.org/spark/\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
    "!tar xf spark-3.4.4-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.4-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession \\\n",
    "  .builder \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99234cb2-52c5-4e2f-8a4a-38fd582e3b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11653,
     "status": "ok",
     "timestamp": 1742584703809,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "PFM9OtDIGSnh",
    "outputId": "6e9c2d15-f13c-47e2-ec29-a1de0cc0a799"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fiind concepute pentru Big Data, un Data Frame nu încarcă datele.\n",
    "El reține doar locația și tipul lor.\n",
    "La fiecare  transformare de date, nou Data Frame va fi creat care reține locația și tipul datelor, precum și lanțul de transformări.\n",
    "Transformările vor fi efectuate doar în momentul scrierii sau afișării datelor de către executori.\n",
    "\n",
    "Crearea unui Data Frame Spark dintr-o listă de Python.\n",
    "\"\"\"\n",
    "\n",
    "data = [\n",
    "    ['Vali', 23, 'Programator', 4, None, ['Sport', 'Boardgames']],\n",
    "    ['Vlad', 34, 'Instalator', 11, None, ['Alergare']],\n",
    "    ['Bea', 29, 'Reporter', 7, True, None]\n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data)\n",
    "\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b7e275-57f9-47cf-9adc-41f9a148e99f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1742586485678,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "Y5WiG6orGSni",
    "outputId": "73c31a92-9f51-481f-83d1-22e59dcd71df"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citirea fișierelor de tip JSON Lines.\n",
    "\"\"\"\n",
    "path_json = '/content/drive/MyDrive/Colab Notebooks/Data/Data/practice/json'\n",
    "data_df_json = spark.read.format('json').load(path_json)\n",
    "\n",
    "#Display rezultate\n",
    "data_df_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca12303-5cc6-4623-a02b-55c344756fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1742586498935,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "uWQMZvZfGSni",
    "outputId": "4940ab40-6342-475a-c4fe-63f180eae98a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citirea fișierelor de tip Parquet.\n",
    "\"\"\"\n",
    "path_parq = '/content/drive/MyDrive/Colab Notebooks/Data/Data/practice/parquet'\n",
    "data_df = spark.read.format('parquet').load(path_parq)\n",
    "\n",
    "#Display rezultate\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01d609b-86a4-469d-afd1-b4a7f1561e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1742586520981,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "mYJbMqdbGSnj",
    "outputId": "a8c26ff1-a29d-4ca2-d61a-9ac012e36e8a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Schema Datelor\n",
    "\n",
    "Întrucât Spark SQL a fost conceput pentru data structurate, el încearcă să determine tipurile de coloane, numită schema datelor,\n",
    " în mod automat, din sursele de date la citire. Acest proces nu este necesar pentru formate structurate ca Parquet,\n",
    "  dar necesar pentru obiecte Python sau date semi-structurate, JSON și CSV.\n",
    "\n",
    "Pentru a dezactiva determinarea automata a tipurilor de date, la citire, se poate furniza schema:\n",
    "\"\"\"\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "data_schema = T.StructType([\n",
    "    T.StructField('nume', T.StringType(), False),\n",
    "    T.StructField('varsta', T.IntegerType(), False),\n",
    "    T.StructField('ocupatie', T.StringType(), False),\n",
    "    T.StructField('vechime',T.IntegerType(),True),\n",
    "    T.StructField('inactiv', T.BooleanType(), True),\n",
    "    T.StructField('extra', T.ArrayType(T.StringType()), True)\n",
    "]);\n",
    "\n",
    "# Dacă se furnizează la citire schema, Spark va ignora sau pune NULL pe coloanele care nu se potrivesc sau nu există\n",
    "\n",
    "data_df = spark.read.format('parquet').schema(data_schema).load(path_parq);\n",
    "\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5739bfd-b4b6-4ace-b79d-0836d70432cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1742586527092,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "3mXhjL7vGSnj",
    "outputId": "09299ed8-c38a-4c0f-b1e9-88e676d5da80"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fie prin detectare automată, fie prin furnizarea la citire, fiecare Data Frame va avea întotdeauna o structură a datelor foarte bine definită. Pentru fiecare coloană avem numele, tipul de dată și dacă se permit valori de ”NULL”.\n",
    "\n",
    "Dacă avem de lucrat cu date care nu au o schemă bine definită, fie date nestructurate, fie date semi-structurate cu multe abateri, atunci NU folosim Data Frame-ul din Spark SQL.\n",
    "\n",
    "Pentru a afișa structură a datelor / schema datelor pentru un Data Frame, folosim:\n",
    "\"\"\"\n",
    "data_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6920adec-eaf4-4a32-9c7d-9e1d782d50d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1742586014630,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "sM_DsT9qGSnk",
    "outputId": "92342f6b-04a6-4c17-9248-e9a192069784"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Frame – Colectarea Datelor\n",
    "\n",
    "Alte metode pe care Spark le ofertă, nu numai pentru depănarea datelor, dar și pentru rarele situații când\n",
    " am redus datele și vrem să prelucrăm cu alte librării, sunt cele de colectare a datelor în obiecte de Python pe mașina Master.\n",
    "\n",
    "Atenție! Datele sunt transferate mai întâi de la executori pe Spark Driver la colectare.\n",
    "Dacă sunt prea multe date, există pericolul ca procesul de Spark Driver să nu mai facă față și să fie terminat de către sistem.\n",
    "\n",
    "Pentru a colecta datele într-o listă de Python, folosim:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_list = data_df.collect()\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d42eb2-541c-4a61-baa0-bc0bed9ed961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "p5oirJ1RGSnk",
    "outputId": "dc17ff45-edee-4359-ae75-6c3a9d6f2dfa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pentru a colecta datele într-un obiect de Data Frame, dar a librăriei Pandas, folosim:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_pandas_pdf = data_df.toPandas()\n",
    "data_pandas_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7917863-50de-48aa-b808-450ba021931a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1742586383180,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "8JUWGtSLGSnl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scrierea Datelor\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Scrierea fișierelor de tip JSON Lines.\n",
    "path_output_json = '/content/drive/MyDrive/Colab Notebooks/Data/Data/output/json'\n",
    "data_df.write.format('json').mode('overwrite').save(path_output_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9ec590-9aa7-4a07-8e92-d8e06f368006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1329,
     "status": "ok",
     "timestamp": 1742586416392,
     "user": {
      "displayName": "Sarchis Dragos",
      "userId": "05832522585147893877"
     },
     "user_tz": -120
    },
    "id": "bNIVrZXVGSnl"
   },
   "outputs": [],
   "source": [
    "# Scrierea fișierelor de tip Parquet.\n",
    "path_output_parq = '/content/drive/MyDrive/Colab Notebooks/Data/Data/output/parquet'\n",
    "data_df.write.format('parquet').mode('overwrite').save(path_output_parq)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Curs 1 - exercise",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
