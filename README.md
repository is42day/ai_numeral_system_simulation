# ğŸ”¥ PySpark Learning Lab

Welcome to the **PySpark Learning Lab**! This repository offers a fully Dockerized, Spark-enabled Python environment tailored for hands-on data processing, analytics, and machine learning with PySpark.


## ğŸš€ Project Features

- ğŸ³ **Docker-Based Development**: Seamlessly set up and manage your development environment using Docker and VS Code Dev Containers.
- ğŸ”¥ **Apache Spark 3.5.5**: Pre-installed with Java 17 for robust big data processing capabilities.
- ğŸ“Š **JupyterLab Integration**: Automatically launches within the container on port `8888`, providing an interactive interface for your data analysis and visualization needs.
- ğŸ§ª **Ready-to-Use PySpark**: PySpark is pre-configured, allowing immediate execution of Spark jobs.
- âš¡ **Efficient Package Management**: Utilizes `uv` for ultrafast Python package installations.
- ğŸ§  **Developer-Friendly Tools**: Comes equipped with linting, autocomplete, notebooks, and a Python terminal to enhance your coding experience.


## ğŸ“ Project Structure
pyspark-learning/ â”œâ”€â”€ notebooks/ # Jupyter Notebooks for lessons & experiments â”œâ”€â”€ scripts/ # PySpark Python scripts â”œâ”€â”€ data/ # Sample datasets (CSV, JSON, Parquet) â”œâ”€â”€ .devcontainer/ # Docker + VS Code configuration â”‚ â”œâ”€â”€ Dockerfile â”‚ â””â”€â”€ devcontainer.json â”œâ”€â”€ requirements.txt # Python dependencies â””â”€â”€ README.md # Project documentation (you're reading it)


## ğŸ› ï¸ Getting Started

### ğŸ§³ Prerequisites

Before you begin, ensure you have the following installed on your system:

- **Docker**: [Download and Install Docker](https://www.docker.com/get-started)
- **Visual Studio Code (VS Code)**: [Download VS Code](https://code.visualstudio.com/)
- **Dev Containers Extension for VS Code**: [Install Dev Containers Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)

### ğŸ› ï¸ Setup Instructions

Follow these steps to set up your development environment:

1ï¸âƒ£ Clone the Repository

Terminal:
git clone https://github.com/YOUR_USERNAME/pyspark-learning.git
cd pyspark-learning

Open the Project in VS Code:

Launch VS Code and open the pyspark-learning directory.
2ï¸âƒ£ Open the Project in VS Code
Launch Visual Studio Code

Open the cloned folder (pyspark-learning)

3ï¸âƒ£ Reopen in Dev Container
On first open, VS Code should prompt:

ğŸ’¡ "Would you like to reopen in a Dev Container?"

âœ… Click "Reopen in Container"

If the prompt doesn't show up:

Press F1 or Ctrl+Shift+P to open the Command Palette

Search and select â¡ï¸ "Dev Containers: Reopen in Container"

â±ï¸ Note: The first time, it may take a few minutes to download and build the image.

4ï¸âƒ£ Access JupyterLab
Once the container is running:

Open your browser

Go to ğŸ‘‰ http://localhost:8888

Youâ€™ll land directly in JupyterLab, ready to code!

5ï¸âƒ£ Explore the Sample Notebooks
Inside JupyterLab, navigate to the ğŸ“ notebooks/ folder

Try the interactive exercises on Spark DataFrames, transformations, and real-world datasets


ğŸ“š Lessons Available
âœ… Lesson 1: SparkSession + DataFrames

ğŸ”œ Lesson 2: Reading CSVs & transforming external data

ğŸ”œ Lesson 3: Writing Parquet + Performance tips

ğŸ”œ Lesson 4: Data Aggregation and Joins

ğŸ”œ Lesson 5: E.ON projects

ğŸ¤ Contributing
Not open for pushing - but feel free to use it as a template and develop your own repo. 

ğŸ” Additional Setup Details
ğŸ“¦ .devcontainer/devcontainer.json
This file defines the development environment for VS Code using Docker.

ğŸ·ï¸ name: Sets the container name to "pyspark-dev" for easier reference.

ğŸ³ dockerFile: Points to the Dockerfile used to build the image.

ğŸ“ context: Sets the build context to the parent directory. Useful if the Dockerfile relies on files outside .devcontainer.

ğŸ§© features: Installs extra tools (like Git) using Dev Containers Features Registry. In this case: ghcr.io/devcontainers/features/git:1.

ğŸ”— mounts: Binds your local workspace folder to /app inside the container with cached consistency for performance.

âš™ï¸ build.args:

ğŸ” BUILDKIT_INLINE_CACHE: Enables Docker layer caching for faster rebuilds.

ğŸŒ HTTP_PROXY & HTTPS_PROXY: Optional proxy settings if you're behind a firewall.

ğŸ§  customizations.vscode.extensions:

ms-python.python: Python support.

ms-toolsai.jupyter: Jupyter notebook support.

ms-python.vscode-pylance: Autocomplete, linting, and IntelliSense.

ğŸšª forwardPorts:

8888: Jupyter Lab access.

4040: Spark UI access.

ğŸ‘¤ remoteUser: Uses "appuser" inside the container for safety (instead of root).

ğŸ³ Dockerfile Breakdown
The Dockerfile defines how the container is built.

ğŸ Base Image:

python:3.9-slim: A lightweight Python image to keep things fast and simple.

ğŸŒ Environment Variables:

SPARK_VERSION, HADOOP_VERSION: Define the versions of Spark and Hadoop to install.

SPARK_HOME: Target installation path for Spark (/opt/spark).

PATH: Adds Spark to the environment for easy CLI access.

ğŸ› ï¸ System Packages Installed:

â˜• openjdk-17-jdk-headless: Java runtime required for Spark.

ğŸŒ curl: For downloading Spark binaries.

ğŸ§ª git: Version control.

ğŸ” openssh-client: For SSH-related tasks.

ğŸ“œ ca-certificates: Ensure SSL works when accessing external APIs.

ğŸ§¹ Cleanup (apt-get clean, rm -rf) to keep image small.

ğŸ”¥ Spark Installation:

Uses curl to download Spark from Apache mirrors and extracts it into /opt/spark.

ğŸ Python Dependencies:

Installs all Python dependencies listed in requirements.txt using pip.

ğŸ‘·â€â™‚ï¸ User Setup:

Adds a non-root user called appuser for better security and development safety.

Sets the working directory to /app and gives appropriate permissions.

âœ… Development Tools & Commands
This project includes a Makefile to simplify formatting, linting, and testing tasks using Ruff and pytest.

ğŸ§¼ Code Formatting & Linting (via Ruff)

Commands

make fmt	    Auto-format all Python files
make fmt-check	Check formatting without changing files
make lint	    Run Ruff to find style/linting issues
make fix	    Fix all autofixable lint issues with Ruff

ğŸ§ª Run Tests (via pytest)
make test	Run all test files via pytest

ğŸ’¬ Bonus Tips
Use ruff check . --select I to organize imports (like isort).

Use pytest -v for verbose test output.

Combine commands for max productivity:
make fmt lint test

ğŸ“œ License
MIT License â€” do whatever you want, just donâ€™t blame me ğŸ˜„
